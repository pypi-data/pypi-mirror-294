services:
    gai-ttt:
        image: kakkoii1337/gai-ttt:latest
        build:
            context: ../gai-ttt-svr
            dockerfile: Dockerfile
        container_name: gai-ttt
        environment:
            DOCKER_BUILDKIT: 1
            LOG_LEVEL: "DEBUG"
            TZ: "Asia/Singapore"
            SWAGGER_URL: "/doc"
            DEFAULT_GENERATOR: "ttt-exllamav2-mistral7b"
        deploy:
            resources:
                reservations:
                    devices:
                        - capabilities: [gpu]
                          driver: nvidia
                          count: all
        ports:
            - "12031:12031"
            - "5678:5678"
        networks:
            - gai-sandbox
        volumes:
            - ~/.gai:/app/.gai

        # Do not use preLaunchTask to run the start container task.
        # Do not use wait-for-client.
        # Both of these will fail because task cannot detect container ready signal.
        # eg. python -m debugpy --listen 0.0.0.0:5678 --wait-for-client main.py
        # Start immediately below without using wait-for-client then attach debugger manually from VS Code.
        command: python -m debugpy --listen 0.0.0.0:5678 main.py
    gai-rag:
        image: kakkoii1337/gai-rag:latest
        container_name: gai-rag
        environment:
            DOCKER_BUILDKIT: 1
            LOG_LEVEL: "DEBUG"
            TZ: "Asia/Singapore"
            SWAGGER_URL: "/doc"
            DEFAULT_GENERATOR: "rag-instructor-sentencepiece"
        deploy:
            resources:
                reservations:
                    devices:
                        - capabilities: [gpu]
                          driver: nvidia
                          count: all
        ports:
            - "12036:12036"
            - "5679:5679"
        networks:
            - gai-sandbox
        volumes:
            - ~/.gai:/app/.gai
    gai-tti:
        image: kakkoii1337/gai-tti:latest
        container_name: gai-tti
        environment:
            CLI_ARGS: "--listen --api --xformers --medvram --no-download-sd-model --ckpt /stable-diffusion-webui/models/Stable-diffusion/runwayml/v1-5-pruned-emaonly.safetensors"
        deploy:
            resources:
                reservations:
                    devices:
                        - capabilities: [gpu]
                          driver: nvidia
                          count: all
        ports:
            - "12035:12035"
        networks:
            - gai-sandbox
        volumes:
            - ~/.gai/models/Stable-diffusion:/stable-diffusion-webui/models/Stable-diffusion
            - ~/.gai/models/VAE:/stable-diffusion-webui/models/VAE

networks:
    gai-sandbox:
        external: true
