import chainlit as cl
from chainlit.input_widget import Select
from litellm import completion
import os
from dotenv import load_dotenv
from PyPDF2 import PdfReader
import docx
from typing import Optional, Dict
import httpx

load_dotenv()

# è¨­ç½®å…¨å±€è®Šé‡
MAX_HISTORY_FOR_LLM = 10  # è¨­ç½®ç™¼é€çµ¦ LLM çš„æœ€å¤§æ­·å²è¨˜éŒ„æ•¸é‡

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "openai/gpt-4o-mini": {
        "model": "openai/gpt-4o-mini",
        "provider": "OpenAI",
        "max_tokens": 4096,  # èª¿æ•´ç‚º gpt-4o-mini çš„æœ€å¤§ token æ•¸
    },
    "gemini/gemini-1.5-pro": {
        "model": "gemini/gemini-1.5-pro",
        "provider": "Gemini",
        "max_tokens": 4096,
    },
    "anthropic/claude-3-sonnet-20240229": {
        "model": "anthropic/claude-3-sonnet-20240229",
        "provider": "Anthropic",
        "max_tokens": 4096,
    },
}


@cl.oauth_callback
def oauth_callback(
    provider_id: str,
    token: str,
    raw_user_data: Dict[str, str],
    default_user: cl.User,
) -> Optional[cl.User]:
    return default_user


@cl.on_chat_start
async def start():
    user = cl.user_session.get("user")
    if not user:
        await cl.Message(content="èªè­‰å¤±æ•—ã€‚è«‹å…ˆç™»éŒ„").send()
        return

    cl.user_session.set("chat_history", [])
    cl.user_session.set("current_model", "openai/gpt-4o-mini")  # é»˜èªä½¿ç”¨ gpt-4o-mini
    cl.user_session.set("file_contents", {})

    # å‰µå»ºæ¨¡å‹é¸æ“‡å™¨
    settings = await cl.ChatSettings(
        [
            Select(
                id="model_selector",
                label="é¸æ“‡æ¨¡å‹",
                values=list(MODEL_CONFIGS.keys()),
                initial_index=list(MODEL_CONFIGS.keys()).index("openai/gpt-4o-mini"),
            )
        ]
    ).send()
    print(f"[chainlit_app.py] user: {user}")
    user_name = ""
    try:
        user_name = user.metadata.get("name", "")
    except:
        user_name = "æ‚¨å¥½"
    await cl.Message(
        content=(
            f"æ­¡è¿ï¼è«‹ä¸Šå‚³æ–‡ä»¶æˆ–ç›´æ¥é–‹å§‹å°è©±ã€‚"
            if user_name == ""
            else f"æ­¡è¿ï¼Œ{user_name}ï¼è«‹ä¸Šå‚³æ–‡ä»¶æˆ–ç›´æ¥é–‹å§‹å°è©±ã€‚"
        )
    ).send()


@cl.on_settings_update
async def setup_agent(settings):
    cl.user_session.set("current_model", settings["model_selector"])


@cl.on_message
async def main(message: cl.Message):
    file_contents = cl.user_session.get("file_contents", {})

    # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šå‚³
    if message.elements:
        for element in message.elements:
            if isinstance(element, cl.File):
                await handle_file_upload(element)

        if not message.content:
            return

    chat_history = cl.user_session.get("chat_history")
    current_model = cl.user_session.get("current_model")
    model_config = MODEL_CONFIGS[current_model]

    # æ§‹å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œåªå–æœ€è¿‘çš„ MAX_HISTORY_FOR_LLM æ¢è¨˜éŒ„ç”¨æ–¼ç™¼é€çµ¦ LLM
    messages_for_llm = []
    for entry in chat_history[-MAX_HISTORY_FOR_LLM:]:
        messages_for_llm.append({"content": entry["user"], "role": "user"})
        messages_for_llm.append({"content": entry["assistant"], "role": "assistant"})

    if file_contents:
        combined_content = "\n\n".join(
            [f"æ–‡ä»¶ '{name}':\n{content}" for name, content in file_contents.items()]
        )
        messages_for_llm.append(
            {
                "content": f"ä»¥ä¸‹æ˜¯ä¸Šå‚³çš„æ–‡ä»¶å…§å®¹ï¼š\n\n{combined_content}\n\nè«‹æ ¹æ“šé€™äº›ä¿¡æ¯å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚",
                "role": "system",
            }
        )

    messages_for_llm.append({"content": message.content, "role": "user"})

    try:
        # ä½¿ç”¨ litellm é€²è¡Œå°è©±ï¼Œå•Ÿç”¨æµå¼å›æ‡‰
        response = completion(
            model=model_config["model"],
            messages=messages_for_llm,
            api_key=os.getenv(f"{current_model.upper()}_API_KEY"),
            stream=True,
            max_tokens=model_config["max_tokens"],  # ä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„ max_tokens
        )

        # åˆå§‹åŒ–æµå¼å›æ‡‰
        msg = cl.Message(content="")
        await msg.send()

        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                full_response += content
                await msg.stream_token(content)

        # å®Œæˆæµå¼å›æ‡‰
        await msg.update()

        # æ›´æ–°èŠå¤©æ­·å²ï¼Œä¿ç•™æ‰€æœ‰è¨˜éŒ„
        chat_history.append({"user": message.content, "assistant": full_response})
        cl.user_session.set("chat_history", chat_history)

    except Exception as e:
        error_message = (
            f"ğŸ™‡â€â™‚ï¸ ç™¼ç”Ÿäº†ä¸€äº›å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«æ”¯æ´åœ˜éšŠã€‚éŒ¯èª¤è©³æƒ…ï¼š{str(e)}"
        )
        await cl.Message(content=error_message).send()


async def handle_file_upload(file: cl.File):
    content = ""
    if file.mime == "application/pdf":
        content = extract_text_from_pdf(file.path)
    elif (
        file.mime
        == "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    ):
        content = extract_text_from_docx(file.path)
    elif file.mime == "text/plain":
        with open(file.path, "r", encoding="utf-8") as f:
            content = f.read()
    else:
        await cl.Message(content=f"ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file.mime}").send()
        return

    file_contents = cl.user_session.get("file_contents", {})
    file_contents[file.name] = content
    cl.user_session.set("file_contents", file_contents)
    await cl.Message(content=f"æ–‡ä»¶ '{file.name}' å·²æˆåŠŸä¸Šå‚³å’Œè™•ç†ã€‚").send()


def extract_text_from_pdf(file_path):
    with open(file_path, "rb") as file:
        reader = PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text


def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    return text
