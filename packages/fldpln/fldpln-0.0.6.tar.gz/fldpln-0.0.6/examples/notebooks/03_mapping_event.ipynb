{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Flood Inundation Mapping\n",
    "\n",
    "This notebook uses the Wildcat Creek (near Manhattan, KS) Labor Day flood event in 2018 to demonstarte how to map the flood event using FLDPLN tiled library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "# import DASK libraries\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import visualize\n",
    "\n",
    "# import the mapping and gauge modules from the fldpln package\n",
    "from fldpln.mapping import *\n",
    "from fldpln.gauge import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Input Tiled Library and Output Folders\n",
    "\n",
    "Here we setup the folder under which tiled libraries (organized as folders) are located. We also setup the output folder (i.e., outputFolder) under which a map folder and a 'scratch' folder are created. The map folder, which is specified later, contains all inundation depth maps. The scratch folder stores temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiled library folder\n",
    "libFolder =  'E:/fldpln/sites/wildcat_10m_3dep/tiled_snz_library' # wildcat\n",
    "# libFolder =  'E:/fldpln/sites/verdigris_10m/tiled_snz_library'\n",
    "\n",
    "# libraries to be mapped\n",
    "allLibNames = ['seglib_py'] # wildcat\n",
    "# allLibNames = ['lib_fldsensing'] # verdigris\n",
    "\n",
    "# Set output folder\n",
    "outputFolder = 'E:/fldpln/sites/wildcat_10m_3dep/maps'\n",
    "# outputFolder = 'E:/fldpln/sites/verdigris_10m/maps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Gauge Stage and Calculate Gauge Depth of Flow (DOF)\n",
    "\n",
    "Here we obtain and prepare flood event stages from stream gauges. The stage at a gauge typically refers to the gauge's datum, which is not necessary of the stream bed elevation which is based on a certain vertical datum. In order to use gauge stage in a FLDPLN library, we need to make sure that gauge stage elevation (gauge + stage) and FSP's filled elevation are based on the same vertical datum. The depth of flow (DOF) at the FSP can then be calculated as the difference. The Wildcat Creek DEM and FLDPLN library are based on the NAVD88 vertical datum. So gauge stage elevations need to be based on the vertical datum too to calculate the DOFs at those gauges. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauge Stage from AHPS and USGS\n",
    "\n",
    "Both USGS and NWS AHPS maintain stream gauges which record past flood stages. There are three AHPS and USGS gauges ([WKCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=wkck1), [MWCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MWCK1), [MSTK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MSTK1)) on the Wildcat Creek that record the 2018 Labor Day flood event. Here we use the maximum Labor Day flood event stages at those gauges to map the maximum inundation extent and depth of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stage from AHPS Historic Crests\n",
    "\n",
    "The flood stage for the 2018 Labor Day flood event in 2018 are available as AHPS historic crests at those gauges [WKCK1](https://water.noaa.gov/gauges/WKCK1), [MWCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MWCK1) and [MSTK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MSTK1). Excel file wildcat_gauges_albers_meters.xlsx has several sheets which store both gauge information (for example, gauge datum) and the event statges with different gauge combinations. The key fields needed for those gauges are: stationid, x, y, and stage_elevation\n",
    "\n",
    "Note that most USGS and AHPS gauge stages are measured in feet and **Make sure that gauge coordinates are in the same coordinate system of the library and gauge stages are also in the same vertical unit of the library.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildcat Creek gauges\n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx' # KS LiDAR DEM in UTM with vertical unit in feet\n",
    "gaugeStageFileName = 'wildcat_gauges_albers_meters.xlsx' # 3DEP DEM in Albers with vertical unit in meters\n",
    "sheetName = 'ThreeGauges' # all 3 gauges\n",
    "# sheetName = 'TwoDsGauges' # 2 downstream gauges\n",
    "# sheetName = 'MSTK1' # the last downstream gauge used in HEC-RAS model\n",
    "\n",
    "# # Verdigris gauges\n",
    "# gaugeStageFileName = 'VerdigrisGauges.xlsx' # CFVK1 is not a USGS gauge and its stage is from AHPS historic crests\n",
    "# sheetName = '2019Flood' # gauges used for 2019 flood \n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFileName, sheet_name=sheetName) \n",
    "# print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stage from USGS NWIS\n",
    "\n",
    "We can also get event maximum stage directly from USGS NWIS to check the historic crests from AHPS. Note that the stages are in feet and we need to convert stages to stage elevation before using it in flood inundation mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildcat Creek 3 USGS gauges (in the order from upstream to downstream)\n",
    "usgsIds = ['06879805','06879810','06879815'] \n",
    "ahpsIds = ['WKCK1','MWCK1','MSTK1']\n",
    "\n",
    "# A period between two dates: Wildcat Creek Sep.3 2018 flood event\n",
    "instStages = GetUsgsGaugeStageFromWebService(usgsIds,startDate='2018-09-02',endDate='2018-09-04')\n",
    "print(instStages)\n",
    "\n",
    "# find the max stage within the time period\n",
    "maxStages = instStages.groupby(['stationid'],as_index=False).agg({'stage_ft':'max'})\n",
    "# find the most recent time with the max stage\n",
    "tdf = pd.merge(instStages, maxStages, how='inner', on=['stationid','stage_ft'])\n",
    "gaugeStagesFromNwis = tdf.groupby(['stationid'], as_index=False).agg({'stationid':'first','stage_ft':'first','stage_time':'max'})\n",
    "print(gaugeStagesFromNwis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Gauge Stage from the National Water Model and HAND\n",
    "\n",
    "HAND FIM uses NWM's discharge and turn it into stage. Here we use HAND reach stage to run FLDPLN for the event. Concepually, we turn reach stage into a synthetic gauge located at the either the mid-point or the outlet of the reach. Selecting the HAND reaches and sythteric gauge location is done by graduate student David Weiss manually for the Wildcat Creek example. Those sytheteic gauges can be treated as USGS/AHPS gauges. The key fields needed are: stationid, x, y, and stage_elevation.  Note that we assume the HAND reach stage elevation is the same as the FLDPLN library DEM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic FSP gauges from NWC reach stage\n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx'\n",
    "# sheetName = 'ReachStageAsDof' \n",
    "gaugeStageFileName = 'wildcat_gauges_albers_meters.xlsx'\n",
    "# sheetName = 'ReachMedianStage' # HAND reach median stage as DOF\n",
    "sheetName = 'ReachOutletStage' # HAND reach outlet stage as DOF\n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFileName, sheet_name=sheetName) # 3 gauges\n",
    "print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap Gauges to FSPs and Calculate Gauge DOF\n",
    "\n",
    "Here we snap gauges (with their stage elevation) to FLDPLN flood source pixels (FSPs), which are the stream pixels. Each snapped gauge FSP has a stream bed elevaltion, which is used to calculate the depth of flow/flood (DOF) at those FSPs. \n",
    "\n",
    "This process also identifies the FLDPLN libraries that the gauges belong to. Note that the same gauges can be snapped to more than one library as FLDPLN libraries may overlap and the overalpping FSPs may have different coordinates! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snap gauges to FSPs on-the-fly\n",
    "print('Snap gauges to FSPs ...')\n",
    "print(f'Number of gauges: {len(gaugeWithStageElevations.index)}')\n",
    "\n",
    "# FLDPLN libraries to whose FSPs gauges are sanpped. All the libraries by default but can be a subset\n",
    "libs2Map = ['seglib_py']\n",
    "\n",
    "# snap the gauges to FSPs. \n",
    "# Fields 'StrOrd','DsDist','SegId','FilledElev'are used for interpolating other FSP DOF\n",
    "# Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP (as there are overlapping FSPs between libraries)!\n",
    "gaugeFspDf = SnapGauges2Fsps(libFolder,libs2Map,gaugeWithStageElevations,snapDist=350,gaugeXField='x',gaugeYField='y',fspColumns=['FspX','FspY','StrOrd','DsDist','SegId','FilledElev']) \n",
    "print(gaugeFspDf)\n",
    "\n",
    "# calculate gauge FSP's DOF\n",
    "gaugeFspDf['Dof'] = gaugeFspDf['stage_elevation'] - gaugeFspDf['FilledElev']\n",
    "\n",
    "# keep only necessary columns for gauge FSPs\n",
    "gaugeFspDf = gaugeFspDf[['lib_name','FspX','FspY','StrOrd','DsDist','SegId','FilledElev','Dof']] # Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP!!!\n",
    "\n",
    "# show info\n",
    "print(f'Number of snapped gauge FSPs: {len(gaugeFspDf)}')\n",
    "# Find libs where the gauges are snapped to, and they are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "print(f'Libraries gauges snapped to: {libs2Map}')\n",
    "print(gaugeFspDf)\n",
    "\n",
    "#\n",
    "# save snapped gauges to CSV file for checking\n",
    "# gaugeFspDf.to_csv(os.path.join(outputFolder, 'SnappedGauges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate FSP's DOF\n",
    "\n",
    "Here we interpolate the DOF for all the FSPs between the gauge-FSPs using their DOF calculated from previous step. The interpolation uses stream orders and starts from low stream order (i.e., main streams) to high stream order (i.e., tributatried). Either horizontal or vertical (by default) interpolation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find libs with snapped gauges. They are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "\n",
    "# prepare the DF for storing interpolated FSP DOF\n",
    "fspDof = pd.DataFrame(columns=['LibName','FspId','Dof'])\n",
    "\n",
    "# prepare DFs for saving interpolated FSPs and their segment IDs\n",
    "fspCols = fspInfoColumnNames + ['Dof']\n",
    "segIdCols = ['SegId','LibName']\n",
    "fsps = pd.DataFrame(columns=fspCols)\n",
    "segIds =pd.DataFrame(columns=segIdCols)\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # interpolate DOF for the gauges\n",
    "    # print('Interpolate FSP DOF using gauge DOF ...')\n",
    "    # fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf) # 'V' by default\n",
    "    fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,weightingType='H') # horizontal interpolation\n",
    "    fspIdDof['LibName'] = libName\n",
    "    # fspDof = fspDof.append(fspIdDof[['LibName','FspId','Dof']], ignore_index=True)\n",
    "    fspDof = pd.concat([fspDof,fspIdDof[['LibName','FspId','Dof']]], ignore_index=True)\n",
    "\n",
    "    # Keep interpolated FSP DOF for saving later\n",
    "    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n",
    "    fspDf = pd.read_csv(fspFile) \n",
    "    fspDf = pd.merge(fspDf,fspDof,how='inner',on=['FspId'])\n",
    "    # fsps = fsps.append(fspDf, ignore_index=True)\n",
    "    fsps = pd.concat([fsps,fspDf], ignore_index=True)\n",
    "    \n",
    "    # Keep FSP segment IDs for saving later\n",
    "    t =  pd.DataFrame(fspDf['SegId'].drop_duplicates().sort_values())\n",
    "    t['LibName'] = libName\n",
    "    # segIds = segIds.append(t, ignore_index=True)\n",
    "    segIds = pd.concat([segIds,t], ignore_index=True)\n",
    "\n",
    "# show interpolated FSPs with Dof\n",
    "print(fspDof)\n",
    "\n",
    "#\n",
    "# save interpolated FSP DOF and their segments for checking. This block of code should be commented out if no-checking needed\n",
    "#\n",
    "# Save DOF and segment IDs to CSV files\n",
    "FspDofFile = os.path.join(outputFolder, 'Interpolated_FSP_DOF.csv')\n",
    "SegIdFile = os.path.join(outputFolder, 'Interpolated_SegIds.csv')\n",
    "fsps.to_csv(FspDofFile, index=False)\n",
    "segIds.to_csv(SegIdFile, index=False)\n",
    "\n",
    "# # turn interpolated sgements into a shapefile\n",
    "# for libName in libs2Map:\n",
    "#     segShp = os.path.join(libFolder, libName, 'stream_orders.shp')\n",
    "#     segs = gpd.read_file(segShp)\n",
    "#     segs['LibName'] = libName\n",
    "#     # print(segs)\n",
    "#     # join by two fields: SegId and LibName\n",
    "#     segDf = pd.merge(segs,segIds,how='inner',on=['SegId','LibName'])\n",
    "#     # print(segDf)\n",
    "#     # write segments as a shapefile\n",
    "#     segDf.to_file(os.path.join(outputFolder, 'Interpolated_Segements.shp'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Flood Inundation Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Mapping Parameters\n",
    "\n",
    "Setup the map folder (i.e., outMapFolderName) which is under the output folder and contains all inundation depth maps. Additional settings include whether to mosaic tiles as single COG file and whether use a Dask local cluster to speed up the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up map folder\n",
    "outMapFolderName = 'wildcat_sep_03' # Wildcat Labor Day flood\n",
    "\n",
    "# Create folders for storing temp and output map files\n",
    "outMapFolder,scratchFolder = CreateFolders(outputFolder,'scratch',outMapFolderName)\n",
    "\n",
    "# whether mosaci tiles as a single COG\n",
    "mosaicTiles = True #True #False\n",
    "\n",
    "# Using LocalCluster by default\n",
    "useLocalCluster = False # This doesn't work on my office desktop though it works fine on KBS server\n",
    "numOfWorkers = round(0.8*os.cpu_count())\n",
    "numOfWorkers = 6\n",
    "print(f'Number of workers: {numOfWorkers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Inundation Depth\n",
    "\n",
    "The process of generating inundation depth map happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mapping info\n",
    "print(f'Tiled FLDPLN library folder: {libFolder}')\n",
    "print(f'Map folder: {outMapFolder}')\n",
    "# Find libs needs mapping\n",
    "libs2Map = fspDof['LibName'].drop_duplicates().tolist()\n",
    "print(f'Libraries to map: {libs2Map}')\n",
    "\n",
    "# check running time\n",
    "startTimeAllLibs = time.time()\n",
    "\n",
    "# create a local cluster to speed up the mapping. Must be run inside \"if __name__ == '__main__'\"!!!\n",
    "if useLocalCluster:\n",
    "    # cluster = LocalCluster(n_workers=4,processes=False)\n",
    "    try:\n",
    "        print('Start a LocalCluster ...')\n",
    "        # NOTE: set worker space (i.e., local_dir) to a folder that the LocalCluster can access. When run the script through a scheduled task, \n",
    "        # the system uses C:\\Windows\\system32 by default, which a typical user doesn't have the access!\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='32GB',local_dir=\"D:/projects_new/fldpln/tools\") # for KARS production server (192G RAM & 8 cores)\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,processes=False) # for KARS production server (192G RAM & 8 cores)\n",
    "        cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='8GB',local_dir=\"E:\\temp\") # for office desktop (64G RAM & 8 cores)\n",
    "        # print('Watch workers at: ',cluster.dashboard_link)\n",
    "        print(f'Number of workers: {numOfWorkers}')\n",
    "        client = Client(cluster)\n",
    "        # print scheduler info\n",
    "        # print(client.scheduler_info())\n",
    "    except:\n",
    "        print('Cannot create a LocalCLuster!')\n",
    "        useLocalCluster = False\n",
    "\n",
    "# dict to store lib processing time\n",
    "libTime={}\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # check running time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # select the FSPs within the lib\n",
    "    fspIdDof = fspDof[fspDof['LibName']==libName][['FspId','Dof']]\n",
    "\n",
    "    # mapping flood depth\n",
    "    if useLocalCluster:\n",
    "        print(f'Map [{libName}] using LocalCLuster ...')\n",
    "        # generate a DAG\n",
    "        dag,dagRoot=MapFloodDepthWithTilesAsDag(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "        if dag is None:\n",
    "            tileTifs = None\n",
    "        else:\n",
    "            # visualize DAG\n",
    "            # visualize(dag)\n",
    "            # Compute DAG\n",
    "            tileTifs = client.get(dag, dagRoot)\n",
    "            if not tileTifs: # list is empty\n",
    "                tileTifs =  None\n",
    "    else:\n",
    "        print(f'Map {libName} ...')\n",
    "        tileTifs = MapFloodDepthWithTiles(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "    print(f'Actual mapped tiles: {tileTifs}')\n",
    "\n",
    "    # Mosaic all the tiles from a library into one tif file\n",
    "    if mosaicTiles and not(tileTifs is None):\n",
    "        print('Mosaic tile maps ...')\n",
    "        mosaicTifName = libName+'_'+outMapFolderName+'.tif'\n",
    "        # Simplest implementation, may crash with very large raster\n",
    "        MosaicGtifs(outMapFolder,tileTifs,mosaicTifName,keepTifs=False)\n",
    "    \n",
    "    # check time\n",
    "    endTime = time.time()\n",
    "    usedTime = round((endTime-startTime)/60,3)\n",
    "    libTime[libName] = usedTime\n",
    "    # print(f'{libName} processing time (minutes):', usedTime)\n",
    "\n",
    "# Show processing time\n",
    "# Individual lib processing time\n",
    "print('Individual library mapping time:', libTime)\n",
    "# total time\n",
    "endTimeAllLibs = time.time()\n",
    "print('Total processing time (minutes):', round((endTimeAllLibs-startTimeAllLibs)/60,3))\n",
    "\n",
    "#\n",
    "# Shutdown local clusters\n",
    "#\n",
    "if useLocalCluster:\n",
    "    print('Shutdown LocalCluster ...')\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "    client.close()\n",
    "    useLocalCluster = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fldpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "acfdeded818a4df3f46e9c2992917120b0a663a2d6c4f04d9d79d65bad4d3fed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
