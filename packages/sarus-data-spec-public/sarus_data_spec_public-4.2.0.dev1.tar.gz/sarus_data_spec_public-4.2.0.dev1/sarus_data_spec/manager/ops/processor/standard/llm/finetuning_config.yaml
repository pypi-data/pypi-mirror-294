# Partial Config for finetuning using QLora
# Most arguments are added via the command line.

gradient_checkpointing: True

quantization:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True

lora:
  lora_attn_modules: ["q_proj","v_proj"]
  apply_lora_to_mlp: False
  apply_lora_to_output: False


#DeepSpeed
deepspeed:
  dtype: 'bf16'
  zero_stage: 1


#Trainer:
trainer:
  learning_rate: 1e-5
  eval_every_n_grad_steps: -1

dp:
  noise_multiplier: 1e-18
  l2_norm_clip: 1e-2
