import logging
from collections.abc import Iterable
from typing import Literal

from fastapi.encoders import jsonable_encoder

# if TYPE_CHECKING:
#     from langchain_core.runnables.config import RunnableConfig
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from sqlmodel import Session

from mtmai.agents.graphchatdemo.weather_node import weather_node
from mtmai.models.chat import MtmChatMessage
from mtmai.mtlibs import aisdk
from mtmai.mtlibs.langgraph import get_langgraph_checkpointer
from mtmai.teams.graph_state import State
from mtmai.teams.tools_node import tools_node

logger = logging.getLogger()


def should_continue(state: State) -> Literal["__end__", "tools", "continue"]:
    messages = state["messages"]
    last_message = messages[-1]

    # if state.get("ask_human"):
    #     return "human"

    # if last_message.tool_calls:
    #     return "tools"
    if not last_message.tool_calls:
        return "end"
    # Otherwise if there is, we continue
    else:
        return "continue"


class GrapphAgent:
    def __init__(self, db: Session, conversation_id: str, user_id: str):
        self.db = db
        self.conversation_id = conversation_id
        self.user_id = user_id

    @property
    def name(self):
        return "grephdemo"

    @property
    def info(self):
        return {"name": "grephdemo"}

    def get_workflow(self) -> CompiledStateGraph:
        workflow = StateGraph(State)
        workflow.add_node("agent", weather_node)
        workflow.add_node("tools", tools_node())

        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                # If `tools`, then we call the tool node.
                "continue": "tools",
                # Otherwise we finish.
                "end": END,
            },
        )
        workflow.add_edge("tools", "agent")
        graph = workflow.compile(
            checkpointer=get_langgraph_checkpointer(),
            # interrupt_before=["tool"],
        )
        return graph

    def handle_chat_messages(self, messages: list[MtmChatMessage]):
        try:
            logger.info("JokeAgent handle Message %s", messages)

            latest_message = messages[-1]
            wf = self.get_workflow()
            result = wf.invoke(input={"topic": latest_message.content})
            logger.info("joke 运行结束 %s", result)
        except Exception as e:
            logger.exception("调用智能体 joke 出错 %s", e)  # noqa: TRY401

    async def chat(
        self, messages: Iterable[ChatCompletionMessageParam], chat_id: str | None = None
    ):
        wf = self.get_workflow()
        thread_id = "22"
        input = State(messages=messages)
        config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
        async for event in wf.astream_events(
            input=input,
            version="v2",
            config=config,
        ):
            kind = event["event"]
            name = event["name"]
            data = event["data"]
            if kind == "on_chat_model_stream":
                # data_chunk: AIMessageChunk = event["data"]["chunk"]
                # content = data_chunk.content
                # print(content + "|")
                # if content:
                #     yield aisdk.text(content)
                print("------")
                print(event["data"]["chunk"].dict())
                content = event["data"]["chunk"].content
                if content:
                    yield aisdk.text(content)
                # yield aisdk.text(content)
            print(f"astream_event: kind: {kind}, name={name},{data}")

            if kind == "on_chain_end" and name == "LangGraph":
                # 完全结束可以拿到最终数据
                # yield f"2: {json.dumps(jsonable_encoder(data))}\n"
                yield aisdk.data(jsonable_encoder(data))
        yield aisdk.finish()
