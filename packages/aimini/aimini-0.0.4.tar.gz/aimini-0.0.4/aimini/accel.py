# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lectures/16_DDPM_v2.ipynb.

# %% auto 0
__all__ = ['MixedPrecision', 'AccelerateCB']

# %% ../nbs/lectures/16_DDPM_v2.ipynb 3
import random
import fastcore.all as fc
from functools import partial
import matplotlib as mpl, numpy as np, matplotlib.pyplot as plt
import logging

from datasets import load_dataset

import torch
from torch import distributions, nn, tensor, optim
from torch.nn import init
from torch.utils.data import DataLoader, default_collate
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy

from diffusers import UNet2DModel

from .conv import def_device
from .datasets import inplace, show_images, show_image
from .learner import MetricsCB, DeviceCB, ProgressCB, DataLoaders, Learner, TrainLearner, SingleBatchCB, MomentumLearner, lr_find, Callback, to_cpu, TrainCB
from .activations import Hooks
from .init import conv, GeneralRelu, init_weights, ActivationStats, set_seed, BatchTransformCB
from .sgd import BatchSchedCB, BaseSchedCB
from .resnet import ResBlock

# %% ../nbs/lectures/16_DDPM_v2.ipynb 45
class MixedPrecision(TrainCB):
    order = DeviceCB.order + 10
    
    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()
    
    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()
    
    def after_loss(self, learn):
        self.autocast.__exit__(None, None, None)
    
    def backward(self, learn):
        self.scaler.scale(learn.loss).backward()
    
    def step(self, learn):
        # self.scaler.unscale_(learn.opt)
        # torch.nn.utils.clip_grad_norm_(learn.model.parameters(), max_norm=1.0)
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/lectures/16_DDPM_v2.ipynb 55
class AccelerateCB(TrainCB):
    order = DeviceCB.order + 10
    
    def __init__(self, n_inp=1, mixed_precision='fp16'):
        super().__init__(n_inp=n_inp)
        self.acc = Accelerator(mixed_precision=mixed_precision)
    
    def before_fit(self, learn):
        learn.model, learn.opt, learn.dls.train, learn.dls.valid = self.acc.prepare(learn.model, learn.opt, learn.dls.train, learn.dls.valid)
    
    def backward(self, learn):  self.acc.backward(learn.loss)
