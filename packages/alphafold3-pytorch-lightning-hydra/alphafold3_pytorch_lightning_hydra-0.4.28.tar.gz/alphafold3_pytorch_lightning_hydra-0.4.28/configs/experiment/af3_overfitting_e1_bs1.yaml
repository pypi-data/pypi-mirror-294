# @package _global_

# lists the experiment parameters corresponding to an "Overfitting Experiment" with one training example and batch size of one

# to execute this experiment run:
# python train.py experiment=af3_overfitting_e1_bs1

defaults:
  - override /callbacks: default
  - override /data: pdb
  - override /logger: wandb
  - override /model: alphafold3
  - override /trainer: deepspeed

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["pdb", "alphafold3", "overfitting", "7a4d", "batch_size_1"]

seed: 12345

# overfitting experiment parameters:

data:
  max_msas_per_chain: 128
  max_templates_per_chain: 20
  num_templates_per_chain: 4
  batch_size: 1
  overfitting_train_examples: true
  sample_only_pdb_ids: [7a4d-assembly1]

logger:
  wandb:
    entity: bml-lab
    group: "af3-overfitting-experiments"
    tags: ${tags}
    name: e1-bs1-${now:%Y%m%d%H%M%S}

model:
  diffusion_add_smooth_lddt_loss: true
  diffusion_add_bond_loss: true
  visualize_val_samples_every_n_steps: 1

  optimizer:
    _target_: deepspeed.ops.adam.DeepSpeedCPUAdam
    _partial_: true
    lr: 1.8e-3
    betas: [0.9, 0.95]
    eps: 1e-8

  net:
    dim_atom: 64
    dim_single: 192
    dim_pairwise: 64
    dim_token: 192
    diffusion_num_augmentations: 12

    checkpoint_input_embedding: true
    checkpoint_trunk_pairformer: true
    checkpoint_distogram_head: true
    checkpoint_confidence_head: true
    checkpoint_diffusion_token_transformer: true

    confidence_head_kwargs:
      pairformer_depth: 1
    template_embedder_kwargs:
      pairformer_stack_depth: 1
    msa_module_kwargs:
      depth: 1
      dim_msa: 32
    pairformer_stack:
      depth: 1
      pair_bias_attn_dim_head: 32
      pair_bias_attn_heads: 8
    diffusion_module_kwargs:
      atom_encoder_depth: 1
      token_transformer_depth: 1
      atom_decoder_depth: 1
      atom_encoder_kwargs:
        attn_pair_bias_kwargs:
          dim_head: 4
      atom_decoder_kwargs:
        attn_pair_bias_kwargs:
          dim_head: 4

trainer:
  min_steps: null
  max_steps: -1
  min_epochs: 1 # NOTE: prevents early stopping
  max_epochs: 20000
  check_val_every_n_epoch: null
  val_check_interval: 50
  log_every_n_steps: 1
  precision: bf16-mixed
