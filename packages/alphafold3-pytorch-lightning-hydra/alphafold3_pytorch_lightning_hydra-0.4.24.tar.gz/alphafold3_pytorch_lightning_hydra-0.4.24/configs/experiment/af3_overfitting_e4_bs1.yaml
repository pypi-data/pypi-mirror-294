# @package _global_

# lists the experiment parameters corresponding to an "Overfitting Experiment" with four training examples and batch size of one

# to execute this experiment run:
# python train.py experiment=af3_overfitting_e4_bs1

defaults:
  - override /callbacks: default
  - override /data: pdb
  - override /logger: wandb
  - override /model: alphafold3
  - override /trainer: fsdp

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags:
  [
    "pdb",
    "alphafold3",
    "overfitting",
    "721p",
    "209d",
    "7a4d",
    "1a6a",
    "batch_size_1",
  ]

seed: 12345

# overfitting experiment parameters:

data:
  max_msas_per_chain: 128
  max_templates_per_chain: 20
  num_templates_per_chain: 4
  batch_size: 1
  overfitting_train_examples: true
  sample_only_pdb_ids:
    [721p-assembly1, 209d-assembly1, 7a4d-assembly1, 1a6a-assembly1]

logger:
  wandb:
    entity: bml-lab
    group: "af3-overfitting-experiments"
    tags: ${tags}
    name: e4-bs1-${now:%Y%m%d%H%M%S}

model:
  diffusion_add_smooth_lddt_loss: true
  diffusion_add_bond_loss: true
  visualize_val_samples_every_n_steps: 1
  net:
    dim_atom: 8
    dim_single: 8
    dim_pairwise: 8
    dim_token: 8
    diffusion_num_augmentations: 4
    confidence_head_kwargs:
      pairformer_depth: 1
    template_embedder_kwargs:
      pairformer_stack_depth: 1
    msa_module_kwargs:
      depth: 1
      dim_msa: 8
    pairformer_stack:
      depth: 12
      pair_bias_attn_dim_head: 4
      pair_bias_attn_heads: 2
    diffusion_module_kwargs:
      atom_encoder_depth: 1
      token_transformer_depth: 1
      atom_decoder_depth: 1
      atom_encoder_kwargs:
        attn_pair_bias_kwargs:
          dim_head: 4
      atom_decoder_kwargs:
        attn_pair_bias_kwargs:
          dim_head: 4

trainer:
  min_steps: null
  max_steps: -1
  min_epochs: 1 # NOTE: prevents early stopping
  max_epochs: 20000
  check_val_every_n_epoch: null
  val_check_interval: 50
  log_every_n_steps: 1
  gradient_clip_val: 0 # NOTE: indicates no clipping is performed, to work around FSDP's clipping limitations
