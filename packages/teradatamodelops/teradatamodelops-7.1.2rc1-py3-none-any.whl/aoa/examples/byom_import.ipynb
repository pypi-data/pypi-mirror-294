{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a connection to ModelOps and making sure we can see some projects\n",
    "\n",
    "from aoa import AoaClient, ProjectApi\n",
    "import pprint\n",
    "\n",
    "client = AoaClient(\n",
    "    aoa_url=\"https://vmo.local/core\",\n",
    "    auth_mode=\"device_code\",\n",
    "    auth_client_id=\"modelops\",\n",
    "    ssl_verify=False,\n",
    "    auth_token_url=(\n",
    "        \"https://vmo.local/sso/realms/teradata/protocol/openid-connect/token\"\n",
    "    ),\n",
    "    auth_device_auth_url=(\n",
    "        \"https://vmo.local/sso/realms/teradata/protocol/openid-connect/auth/device\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "project_api = ProjectApi(aoa_client=client)\n",
    "\n",
    "projects = list(project_api)\n",
    "pprint.pprint(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice a file could be with any name, we just upload it as model.pmml\n",
    "file = \"./pima.pmml\"\n",
    "language = \"PMML\"\n",
    "client.project_id = \"23e1df4b-b630-47a1-ab80-7ad5385fcd8d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a dataset template\n",
    "\n",
    "from aoa import DatasetTemplateApi\n",
    "\n",
    "dataset_template_api = DatasetTemplateApi(aoa_client=client)\n",
    "\n",
    "dataset_template = dataset_template_api.find_by_name_like(\"PIMA\")[\"_embedded\"][\n",
    "    \"datasetTemplates\"\n",
    "][0]\n",
    "dataset_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using that dataset template, let's fetch datasets ...\n",
    "\n",
    "from aoa import DatasetApi\n",
    "\n",
    "dataset_api = DatasetApi(aoa_client=client)\n",
    "datasets = dataset_api.find_by_dataset_template_id(dataset_template[\"id\"])[\"_embedded\"][\n",
    "    \"datasets\"\n",
    "]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and take train dataset\n",
    "\n",
    "train_dataset = [d for d in datasets if d[\"scope\"] == \"train\"][0]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API, allows users do fetch a default dataset connection\n",
    "# (notice it only works for real users, service account probably don't have personal connections)\n",
    "\n",
    "from aoa import UserAttributesApi\n",
    "\n",
    "user_attributes_api = UserAttributesApi(aoa_client=client)\n",
    "default_connection = user_attributes_api.get_default_connection()\n",
    "default_connection\n",
    "default_connection_id = default_connection[\"value\"][\"defaultDatasetConnectionId\"]\n",
    "default_connection_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a BYOM model that we'll use to register model versions in\n",
    "\n",
    "from aoa import ModelApi\n",
    "import uuid\n",
    "\n",
    "model_api = ModelApi(aoa_client=client)\n",
    "model_dict = {\n",
    "    \"name\": f\"{language}_Python_{uuid.uuid4().clock_seq}\",\n",
    "    \"description\": f\"{language} model defined from Python SDK\",\n",
    "    \"language\": language,\n",
    "}\n",
    "model_response = model_api.save(model_dict)\n",
    "model = model_response[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing that artefact, all we need is a language/format, and path to a local file\n",
    "\n",
    "from aoa import TrainedModelArtefactsApi\n",
    "\n",
    "trained_model_artefacts_api = TrainedModelArtefactsApi(aoa_client=client)\n",
    "import_id = trained_model_artefacts_api.upload_byom_model(\"PMML\", file)\n",
    "import_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step of BYOM is here. We define a structure with all necessary metadata and submit our import request\n",
    "\n",
    "import_request = {\n",
    "    \"artefactImportId\": import_id,\n",
    "    \"externalId\": str(uuid.uuid4()),\n",
    "    \"modelMonitoring\": {\n",
    "        \"language\": language,\n",
    "        \"useDefaultEvaluation\": True,\n",
    "        \"evaluationEnabled\": True,\n",
    "        \"modelType\": \"CLASSIFICATION\",\n",
    "        \"byomColumnExpression\": (\n",
    "            \"CAST(CAST(json_report AS JSON).JSONExtractValue('$.predicted_HasDiabetes')\"\n",
    "            \" AS INT)\"\n",
    "        ),\n",
    "        \"driftMonitoringEnabled\": True,\n",
    "        \"datasetId\": train_dataset[\"id\"],\n",
    "        \"datasetConnectionId\": default_connection_id,\n",
    "    },\n",
    "}\n",
    "\n",
    "response = model_api.import_byom(model, import_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
